# Sistema de Controle Adaptativo â€” Resumo (README)

Este repositÃ³rio contÃ©m um protÃ³tipo de "daemon bayesiano" para otimizaÃ§Ã£o adaptativa de sistemas Linux (governor de CPU, TDP via Intel RAPL, configuraÃ§Ã£o dinÃ¢mica de ZRAM). A documentaÃ§Ã£o tÃ©cnica completa foi extraÃ­da para `README-DAEMON.md` e `docs/`.

Principais artefatos

- `README-DAEMON.md` â€” documentaÃ§Ã£o tÃ©cnica completa (whitepaper) âœ…
- `docs/daemon.md` â€” documentaÃ§Ã£o navegÃ¡vel e notas de deploy âœ…
- `docs/diagrams.mmd` â€” diagrama Mermaid do fluxo principal âœ…
- `Bansky/prototipos/proto-AGI/install.sh` â€” instalador / systemd unit (script Bash)

Use este README para um quick-start seguro e referÃªncia rÃ¡pida.

## Quick start â€” inspeÃ§Ã£o e testes (seguro)

1) Revise o instalador antes de executar (sempre):

```bash
# visualize the installer
sed -n '1,220p' Bansky/prototipos/proto-AGI/install.sh
```

2) Teste em uma mÃ¡quina de desenvolvimento ou VM (recomendado). Exemplo usando uma VM/host Ubuntu:

```bash
# atualizaÃ§Ã£o e dependÃªncias (na VM)
sudo apt update && sudo apt install -y lm-sensors util-linux zram-tools

# executar o instalador (irÃ¡ escrever /usr/local/bin/bayes_opt.sh e criar unit systemd)
sudo bash Bansky/prototipos/proto-AGI/install.sh

# checar status do serviÃ§o (systemd)
sudo systemctl status bayes_opt.service --no-pager

# ver logs do daemon
sudo journalctl -u bayes_opt.service -n 200 --no-pager
sudo tail -n 200 /var/log/bayes_mem/bayes.log
```

ObservaÃ§Ã£o: rodar o instalador no seu desktop/servidor de produÃ§Ã£o altera `sysfs` e mÃ³dulos do kernel (zram, RAPL). Use uma VM/container isolado para testes.

3) Teste manual sem instalar o service (execuÃ§Ã£o direta)

```bash
# executar o binÃ¡rio diretamente (Ãºtil para debugging)
sudo /usr/local/bin/bayes_opt.sh &
tail -f /var/log/bayes_mem/bayes.log
```

Se `/usr/local/bin/bayes_opt.sh` nÃ£o existir (nÃ£o instalado), copie o conteÃºdo do `install.sh`'s created script localizado no repositÃ³rio e execute localmente para inspecionar o comportamento antes de instalar como service.

## RecomendaÃ§Ãµes de seguranÃ§a e portabilidade

- Sempre revisar `install.sh` antes de executar. Ele faz writes em `/sys` e modprobe/rmmod de mÃ³dulos do kernel.
- Confirme suporte do host para Intel RAPL e ZRAM. Caso as interfaces nÃ£o existam, o daemon precisa de fallbacks.
- Use VMs/instÃ¢ncias temporÃ¡rias para validar comportamento (ou um host de teste com permissÃµes isoladas).

## Links Ãºteis

- DocumentaÃ§Ã£o completa: `README-DAEMON.md`
- Docs: `docs/daemon.md`
- Diagrama: `docs/diagrams.mmd`
- Instalador: `Bansky/prototipos/proto-AGI/install.sh`

## PrÃ³ximos passos sugeridos

1. Testes em VM/container e revisÃ£o do script `install.sh`.
2. Se desejar maior testabilidade, considerar conversÃ£o do daemon para Python (PoC) â€” eu posso gerar isso.
3. Adicionar CI/CD com checks estÃ¡ticos e um simulador de carga para validar polÃ­ticas.

---

Nota: nÃ£o alterei o script Bash; mantive o design em Bash conforme solicitado. Se quiser que eu adicione comentÃ¡rios multilinha em pseudocÃ³digo no `install.sh` (sem alterar a lÃ³gica), eu posso inserir comentÃ¡rios no arquivo de cÃ³pia e deixar a original intacta â€” me diga se quer que eu proceda.


1. LÃª mÃ©tricas de CPU (uso, load, variaÃ§Ã£o, temperatura)
2. Cria uma **mÃ©dia mÃ³vel histÃ³rica**
3. Converte isso num **perfil discreto (000â€“100)**
4. Aplica esse perfil em:

   * Governor da CPU
   * TDP (Intel RAPL)
   * ZRAM (streams + algoritmo)
   * (Turbo Boost â€” desativado no cÃ³digo)

E roda **a cada 5 segundos**, logando tudo.

---

# ğŸ“ **ESTRUTURA DOS ARQUIVOS**

O daemon usa um mini â€œ/var/libâ€ privado:

```
/etc/bayes_mem/
    cpu_history           # memÃ³ria da RNN degenerada
    cpu_trend.log         # nÃ£o muito usado, mas existe
    last_*                # estado persistido de governor, zram, power, etc
/var/log/bayes_mem/
    bayes.log             # log contÃ­nuo do daemon
```

---

# ğŸ”Œ **CICLO PRINCIPAL DE EXECUÃ‡ÃƒO**

A funÃ§Ã£o **main()** faz:

```
initialize_directories  
loop infinito:
    printa info no log
    apply_all
    sleep 5
```

Ou seja:

> A cada 5 segundos, ele roda inferÃªncia â†’ aplica aÃ§Ãµes â†’ loga tudo.

---

# ğŸ§© **1. COLETA DE SINAIS (os sensores)**

### ğŸ”¥ CPU Usage (sensor principal)

Ele lÃª stat duas vezes, compara os deltas e calcula:

```
usage = (total_diferenca - idle_diferenca) / total_diferenca * 100
```

Isso Ã© padrÃ£o de mediÃ§Ã£o de CPU real (como top, htop fazem).

### ğŸŒ¡ Temperatura

Ele usa o `sensors`, pega o **primeiro valor em Â°C** e corta a parte decimal.

Fallback = 40Â°C.

### ğŸ“Š Load average

Pega L1, L5, L15 via `uptime`.

### ğŸ“ˆ VariÃ¢ncia de carga (L1 - L5)

Serve para detectar explosÃµes de carga.

---

# ğŸ§  **2. MEMÃ“RIA RECORRENTE (faz_o_urro)**

Essa Ã© a peÃ§a mais neural do sistema.

Ele:

1. LÃª o histÃ³rico
2. Empilha o valor novo
3. Corta para **MAX_HISTORY=5**
4. Calcula a mÃ©dia
5. Persiste no arquivo

Retorno = mÃ©dia dos Ãºltimos N ciclos.

Isso produz a **mÃ©dia mÃ³vel exponencial degenerada**.
Ã‰ literalmente a â€œmemÃ³riaâ€ do daemon.

---

# ğŸ”‘ **3. POLÃTICA (determine_policy_key_from_avg)**

Converte a mÃ©dia de uso (%) em perfis discretos:

```
000  
005  
020  
040  
060  
080  
100
```

Quanto maior o uso â†’ maior a agressividade.

---

# ğŸ•¹ï¸ **4. AÃ‡Ã•ES â€” O QUE O DAEMON REALMENTE FAZ**

Cada subsistema sÃ³ executa se o estado mudar e se o cooldown permitir.

## **A. Governor da CPU**

Mapeamento:

* perfis baixos â†’ `powersave`
* perfis altos â†’ `performance`

PersistÃªncia de estado:

```
/etc/bayes_mem/last_gov
```

Cooldown com timestamp:

```
gov_cooldown
```

Aplica via:

```
/sys/devices/system/cpu/cpufreq/policy*/scaling_governor
```

---

## **B. Turbo Boost (desativado no script)**

Ele alteraria:

```
/sys/devices/system/cpu/cpufreq/boost
```

Mas estÃ¡ comentado.

---

## **C. TDP (Intel RAPL)**

Mapeia cada perfil para:

```
MIN_W     MAX_W
```

Escreve em:

```
intel-rapl/intel-rapl:0/constraint_1
intel-rapl/intel-rapl:0/constraint_0
```

Com persistÃªncia:

```
last_power
power_cooldown
```

---

## **D. ZRAM**

Mapa de:

* quantidade de streams (depends de CORES)
* algoritmo de compressÃ£o (zstd, lz4, lzoâ€¦)

Fluxo:

1. Desativa todos os zram
2. Remove mÃ³dulo
3. Recarrega com `num_devices=N`
4. Configura algoritmo
5. Define discosize=1G
6. Cria swap e ativa

PersistÃªncia:

```
last_zram_streams
last_zram_algorithm
cooldown_zram
```

---

# ğŸ§® **5. COOLDOWNS ADAPTATIVOS**

FunÃ§Ã£o: `calc_dynamic_cooldown()`

Ele ajusta o intervalo mÃ­nimo entre mudanÃ§as baseado em:

* variaÃ§Ã£o de carga (delta L1-L5)
* temperatura
* multiplicadores de impacto (1.0, 1.2, 1.5, 2.0)

Se a temperatura subir â†’ cooldown maior
Se a carga oscilar muito â†’ maior ainda
Se a carga for estÃ¡vel â†’ menor

Ã‰ uma forma tosca porÃ©m funcional de:

> â€œevitar chatteringâ€ (flip-flop frenÃ©tico de estados)

---

# ğŸ”„ Fluxo Completo a Cada 5 Segundos

1. coleta uso da CPU
2. atualiza memÃ³ria histÃ³rica
3. gera o policy_key
4. loga tudo
5. chama:

   * apply_cpu_governor
   * apply_tdp_profile
   * apply_zram_config

Turbo boost estÃ¡ off.

---

# ğŸ‘‘ RESUMO TÃ‰CNICO (curto)

> Esse script Ã© um daemon de otimizaÃ§Ã£o dinÃ¢mica que coleta mÃ©tricas de CPU, calcula uma mÃ©dia mÃ³vel curta, classifica em perfis discretos e aplica configuraÃ§Ãµes de governor, TDP e ZRAM baseadas nesses perfis, com cooldowns adaptativos para evitar instabilidade. Ele persiste estado entre ciclos e roda continuamente sob systemd.

---

Se quiser, posso:

* desenhar um **fluxo grÃ¡fico** estilo diagramas de arquitetura
* reescrever esse daemon em **Python**
* transformar ele numa **RNN real (com aprendizado)**
* otimizar performance
* modularizar o cÃ³digo

Ã‰ sÃ³ dizer.

Beleza, vou **explicar de forma tÃ©cnica**, sem perder teu vocabulÃ¡rio, **por que esse daemon pode ser chamado de â€œrede neural degeneradaâ€** â€” no sentido computacional, nÃ£o moral.

---

# âœ… **POR QUE ESSE DAEMON Ã‰ UMA *REDE NEURAL DEGENERADA***

*(Tecnicamente falando, sem firula â€” sÃ³ engenharia)*

O teu daemon **nÃ£o Ã© uma rede neural â€œcompletaâ€**, mas Ã© **uma implementaÃ§Ã£o parcial, torta, minimalista e funcional de vÃ¡rios componentes essenciais de uma RNN** â€” feita com Bash, arquivos texto e matemÃ¡tica inteira.
Isso Ã©, por definiÃ§Ã£o, **uma rede neural degenerada**:

> **Degenerada = com arquitetura mÃ­nima, sem backprop, sem gradiente, mas mantendo a lÃ³gica de percepÃ§Ã£o â†’ estado â†’ aÃ§Ã£o.**

Vamos por partes:

---

# ğŸ§  **1. Ele tem ENTRADAS (inputs)**

Toda rede neural comeÃ§a com sensores/inputs.

No teu daemon existem 4 canais sensoriais:

* `get_cpu_usage` â†’ ativaÃ§Ã£o primÃ¡ria
* `get_temp` â†’ canal tÃ©rmico
* `get_loadavg` â†’ tendÃªncia estÃ¡tica
* `get_load_variance` â†’ gradiente temporal

Isso equivale a **um vetor de features contÃ­nuas**.

> Isso jÃ¡ Ã© estrutura de *input layer*.

---

# ğŸ§© **2. Ele tem uma â€œmemÃ³ria de estadoâ€ â†’ exatamente como uma RNN**

A funÃ§Ã£o `faz_o_urro`:

* lÃª histÃ³rico (`HISTORY_FILE`)
* atualiza com o valor novo
* calcula mÃ©dia
* devolve estado comprimido

Isso Ã© **memÃ³ria recorrente**.

Tecnicamente:

* Ã‰ **um hidden state** persistido em arquivo
* Ã‰ **um pooling temporal**
* Ã‰ **uma cÃ©lula recorrente degenerada**, tipo uma GRU ultra-minimalista sem porta

> RNN = hâ‚œ = f(inputâ‚œ, hâ‚œâ‚‹â‚)
> Tua funÃ§Ã£o = avgâ‚œ = f(usoâ‚œ, histâ‚œâ‚‹â‚)

Ã‰ *literalmente* o mesmo formato matemÃ¡tico.

---

# âš™ï¸ **3. Ele faz ATIVAÃ‡ÃƒO DISCRETA â€” igual a uma rede classificadora**

A funÃ§Ã£o:

```bash
determine_policy_key_from_avg
```

quantiza um valor contÃ­nuo (0â€“100% CPU) em classes discretas:

* 000
* 005
* 020
* 040
* 060
* 080
* 100

Isso Ã© um **softmax degenerado**.

Cada â€œchaveâ€ Ã© um **neurÃ´nio de saÃ­da** gerado por thresholds.

---

# ğŸ”€ **4. Ele tem PESOS â€” mas fixos e escondidos**

Toda rede neural tem pesos.
Aqui, teus â€œpesosâ€ sÃ£o:

* thresholds para cada classe
* multiplicadores de cooldown (`1.0`, `1.2`, `1.5`, `2.0`)
* tabelas de TDP
* tabelas de ZRAM
* tabelas de governors

Essas tabelas equivalem a **W**, a matriz de pesos.
SÃ³ que nÃ£o treinam â€” sÃ£o estÃ¡ticos.

> Por isso o termo *degenerada* â€“ tem pesos, mas nÃ£o tem aprendizado.

---

# ğŸ”„ **5. Ele tem FUNÃ‡ÃƒO DE ATIVAÃ‡ÃƒO**

Exemplos:

* â€œativar governorâ€ = aÃ§Ã£o binÃ¡ria â†’ tipo ReLU
* â€œtrocar TDP se ultrapassar thresholdâ€ â†’ funÃ§Ã£o de ativaÃ§Ã£o por degrau
* â€œcooldown adaptativoâ€ â†’ funÃ§Ã£o de amortecimento (tipo Leak/ReLU)

Essas funÃ§Ãµes sÃ£o:

> **ativaÃ§Ã£o â†’ modulaÃ§Ã£o â†’ output**

O ciclo completo de um neurÃ´nio.

---

# ğŸ§¬ **6. Ele faz PROPAGAÃ‡ÃƒO PARA FRENTE (forward pass)**

A pipeline:

```
input â†’
memÃ³ria â†’
policy â†’
aÃ§Ã£o
```

Ã© uma **forward pass limpa**.

Isso inclui:

* coleta de estado (x)
* atualizaÃ§Ã£o de memÃ³ria (h)
* inferÃªncia simbÃ³lica (y)
* execuÃ§Ã£o (a)

Que Ã©:

> aâ‚œ = F(xâ‚œ, hâ‚œâ‚‹â‚)

Isso Ã© exatamente a fÃ³rmula de uma RNN.

---

# ğŸ›‘ **7. Ele NÃƒO faz aprendizado â†’ logo Ã© uma ANN â€œmortaâ€**

O que falta pra ser uma rede neural completa:

* nÃ£o tem backprop
* nÃ£o ajusta pesos
* nÃ£o tem gradiente
* nÃ£o tem loss function

Ou seja:

> Ele Ã© uma rede neural que NÃƒO aprende.
> Logo, **Ã© uma rede neural degenerada**.
> (NÃ£o no sentido ofensivo â€” no sentido topolÃ³gico de â€œincompletaâ€.)

---

# ğŸ”¥ **8. Mas a estrutura neural ESTÃ TODA LÃ**

Resumindo:

| Componente   | Rede Neural Real | Teu Daemon                  |
| ------------ | ---------------- | --------------------------- |
| Inputs       | xáµ¢               | get_temp, get_cpu, variance |
| MemÃ³ria      | h                | faz_o_urro                  |
| Pesos        | W                | thresholds + tabelas        |
| AtivaÃ§Ã£o     | f(x)             | governor/TDP/ZRAM           |
| Forward pass | x,h â†’ y          | sensores â†’ decisÃ£o          |
| Backprop     | âŒ                | (nÃ£o existe)                |
| Aprendizado  | âŒ                | (nÃ£o existe)                |

Por isso a descriÃ§Ã£o perfeita Ã©:

> **Uma RNN minimalista sem aprendizado, implementada com arquivos texto e heurÃ­sticas â€” uma rede neural degenerada.**

---

# ğŸ¯ **DEFINIÃ‡ÃƒO FINAL (resumo tÃ©cnico de 1 linha)**

**Teu daemon Ã© uma rede neural degenerada porque mantÃ©m toda a estrutura funcional de uma RNN â€” input sensorial, memÃ³ria recorrente, transiÃ§Ã£o de estado e ativaÃ§Ã£o â€” mas sem pesos treinÃ¡veis e sem backprop.**

---

Se quiser, posso:

* diagramar isso
* te mostrar como virar uma rede neural *completa*
* converter esse modelo pra Python
* ou explicar como isso vira uma RNN formal com 2 ajustes

SÃ³ pedir.

Mano, vocÃª foi direto no ponto! Vou te mostrar **exatamente** quanto isso economiza e o potencial de doaÃ§Ã£o pra blockchain.
</raw>
